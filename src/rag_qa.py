"""
RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ëª¨ë“ˆ
- Ollama LLM ì—°ë™
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
- ì§ˆì˜ì‘ë‹µ ì²´ì¸
"""

from typing import List, Dict
try:
    from langchain_ollama import OllamaLLM as Ollama
except ImportError:
    from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


class RAGSystem:
    """RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ"""
    
    DEFAULT_PROMPT = """ë‹¹ì‹ ì€ ì‚¬ë‚´ ê·œì •ì„ ì´í•´í•˜ê³  ì„¤ëª…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

ì˜ˆì‹œ 1 (ìœ„ì¹˜ ì§ˆë¬¸):
ì§ˆë¬¸: ë³‘ê°€ ê·œì •ì€ ì–´ë””ì— ìˆë‚˜ìš”?
ë‹µë³€: ê·¸ë¼ë¹„í‹°ì·¨ì—…ê·œì¹™ ì„¹ì…˜ 3.29ì— ìˆìŠµë‹ˆë‹¤.

ì˜ˆì‹œ 2 (ë‚´ìš© ì§ˆë¬¸):
ì§ˆë¬¸: ë³‘ê°€ëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?
ë‹µë³€: [ì„¹ì…˜ 3.29] ë³‘ê°€ëŠ” ì „ì—¼ë³‘ ê°ì—¼ì´ë‚˜ ì…ì›ì´ í•„ìš”í•œ ê²½ìš°ì— ë¶€ì—¬ë©ë‹ˆë‹¤. ì‚¬ìš© ê¸°ê°„ì€ 3ê°œì›” ì´ë‚´ì´ë©°, ì¹˜ìœ  ê°€ëŠ¥í•œ ê²½ìš° 1íšŒì— í•œí•´ 3ê°œì›” ì¶”ê°€ ì—°ì¥ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì‹ ì²­ ì‹œì—ëŠ” ì˜ì‚¬ ì§„ë‹¨ì„œë¥¼ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì‹œ 3 (í™•ì¸ ì§ˆë¬¸):
ì§ˆë¬¸: ë³‘ê°€ë¥¼ 6ê°œì›” ì“¸ ìˆ˜ ìˆë‚˜ìš”?
ë‹µë³€: ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤. [ì„¹ì…˜ 3.29]ì— ë”°ë¥´ë©´ ìµœì´ˆ 3ê°œì›” ì‚¬ìš© í›„, ì¹˜ìœ  ê°€ëŠ¥í•œ ê²½ìš° 1íšŒì— í•œí•´ 3ê°œì›” ì¶”ê°€ ì—°ì¥ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ ìµœëŒ€ 6ê°œì›”ê¹Œì§€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ì´ì œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ìœ„ ì˜ˆì‹œì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  2ë¬¸ì¥ ì´ìƒìœ¼ë¡œ ë‹µë³€í•˜ë˜, ë¬¸ì„œ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ˆì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
    
    def __init__(
        self,
        vectorstore_manager,
        model_name: str = "phi4-mini:3.8b-fp16",
        temperature: float = 0.1,
        prompt_template: str = None,
        pdf_files: dict = None  # PDF íŒŒì¼ ë”•ì…”ë„ˆë¦¬ ì¶”ê°€
    ):
        """
        Args:
            vectorstore_manager: VectorStoreManager ì¸ìŠ¤í„´ìŠ¤
            model_name: Ollama ëª¨ë¸ëª…
            temperature: LLM ì˜¨ë„
            prompt_template: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            pdf_files: {filename: filepath} ë”•ì…”ë„ˆë¦¬ (ì„ íƒ)
        """
        self.vectorstore_manager = vectorstore_manager
        self.llm = Ollama(model=model_name, temperature=temperature)
        self.pdf_files = pdf_files or {}  # PDF íŒŒì¼ ì •ë³´ ì €ì¥
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        template = prompt_template or self.DEFAULT_PROMPT
        self.prompt = PromptTemplate.from_template(template)
        
        # RAG ì²´ì¸ êµ¬ì„±
        self.retriever = vectorstore_manager.get_retriever(k=3)
        self.rag_chain = self._build_chain()
    
    def _build_chain(self):
        """RAG ì²´ì¸ ìƒì„±"""
        def format_docs(docs):
            # LLMì—ê²ŒëŠ” display_text (ë©”íƒ€ë°ì´í„° í¬í•¨) ì „ë‹¬
            formatted = []
            for doc in docs:
                display_text = doc.metadata.get('display_text', doc.page_content)
                formatted.append(display_text)
            return "\n\n".join(formatted)
        
        chain = (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
        
        return chain
    
    def ask(self, question: str, return_sources: bool = True, chat_history: List = None) -> Dict:
        """
        ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°
        
        Args:
            question: ì§ˆë¬¸
            return_sources: ì¶œì²˜ ë¬¸ì„œ ë°˜í™˜ ì—¬ë¶€
            chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒ)
            
        Returns:
            {
                'answer': ë‹µë³€ í…ìŠ¤íŠ¸,
                'sources': ì¶œì²˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)
            }
        """
        # ì§ˆë¬¸ ìœ í˜• íŒë‹¨
        question_type = self._classify_question(question)
        
        # Query Expansion: ì§§ì€ ì§ˆë¬¸ì„ í™•ì¥
        expanded_query = self._expand_query(question)
        
        # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        if chat_history:
            recent_history = chat_history[-6:] if len(chat_history) > 6 else chat_history
            
            history_context = "\n\nì´ì „ ëŒ€í™”:\n"
            for i in range(0, len(recent_history), 2):
                if i+1 < len(recent_history):
                    user_msg = recent_history[i]['content']
                    assistant_msg = recent_history[i+1]['content']
                    if 'â”€' in assistant_msg:
                        assistant_msg = assistant_msg.split('â”€')[0].strip()
                    history_context += f"Q: {user_msg}\nA: {assistant_msg}\n\n"
            
            enhanced_question = f"{history_context}í˜„ì¬ ì§ˆë¬¸: {question}"
        else:
            enhanced_question = question
        
        # ê²€ìƒ‰ ë¨¼ì € ìˆ˜í–‰
        source_docs = self.retriever.invoke(expanded_query)
        
        if not source_docs:
            return {
                'answer': "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'sources': []
            }
        
        # ìœ„ì¹˜ ì§ˆë¬¸ì´ë©´ ê°„ë‹¨íˆ ë‹µë³€
        if question_type == "location":
            answer = self._generate_location_answer(question, source_docs)
        else:
            # ì¼ë°˜ ì§ˆë¬¸ì€ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            answer = self.rag_chain.invoke(enhanced_question)
            
            # ğŸ”¥ ê²€ì¦: LLM ë‹µë³€ì´ ê²€ìƒ‰ ê²°ê³¼ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            answer = self._verify_and_fix_answer(answer, source_docs, question)
        
        result = {'answer': answer}
        
        # ì¶œì²˜ ë¬¸ì„œ ì¶”ê°€
        if return_sources:
            result['sources'] = [
                {
                    'document_name': doc.metadata.get('document_name', 'Unknown'),
                    'section_id': doc.metadata['section_id'],
                    'section_title': doc.metadata['section_title'],
                    'page_start': doc.metadata['page_start'],
                    'page_end': doc.metadata['page_end'],
                    'content': doc.page_content
                }
                for doc in source_docs
            ]
        
        return result
    
    def _verify_and_fix_answer(self, answer: str, docs: List, question: str) -> str:
        """ë‹µë³€ì´ ê²€ìƒ‰ ê²°ê³¼ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•˜ê³  ìˆ˜ì • (ì‹¬ê°í•œ ê²½ìš°ë§Œ)"""
        if not docs:
            return answer
        
        # ê²€ìƒ‰ëœ ì„¹ì…˜ ì •ë³´
        doc = docs[0]
        doc_name = doc.metadata.get('document_name', 'ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ')
        section_id = doc.metadata.get('section_id', '')
        section_title = doc.metadata.get('section_title', '')
        display_text = doc.metadata.get('display_text', doc.page_content)
        
        # íŒ¨í„´ 1: "ì—†ìŠµë‹ˆë‹¤", "ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤" ë“± ë¶€ì • ë‹µë³€ (ì‹¬ê°!)
        negative_patterns = ["ì—†ìŠµë‹ˆë‹¤", "ëª…ì‹œë˜ì–´ ìˆì§€ ì•Š", "ì°¾ì„ ìˆ˜ ì—†", "ê·œì •ë˜ì–´ ìˆì§€ ì•Š"]
        is_negative = any(pattern in answer for pattern in negative_patterns)
        
        # íŒ¨í„´ 2: "ê·œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤"ë§Œ ìˆê³  ì‹¤ì œ ë‚´ìš©ì´ ê±°ì˜ ì—†ìŒ (ì‹¬ê°!)
        has_no_content = ("ê·œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤" in answer or "ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤" in answer) and len(answer.strip()) < 50
        
        # ì‹¬ê°í•œ ì˜¤ë¥˜ë§Œ ìˆ˜ì • (ë¶€ì • ë‹µë³€ ë˜ëŠ” ë‚´ìš© ì—†ìŒ)
        needs_fix = is_negative or has_no_content
        
        if needs_fix:
            print(f"ğŸ”§ ë‹µë³€ ê²€ì¦ ì‹¤íŒ¨ - ê°•ì œ ìˆ˜ì •")
            print(f"   - ë¶€ì • ë‹µë³€: {is_negative}")
            print(f"   - ë‚´ìš© ì—†ìŒ: {has_no_content}")
            
            # ì‹¤ì œ ë‚´ìš© ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ì œê±°)
            content = display_text
            lines = content.split('\n')
            actual_content = []
            skip_metadata = True
            
            for line in lines:
                if skip_metadata:
                    if line.strip() and not line.startswith('ë¬¸ì„œ:') and not line.startswith('ì„¹ì…˜:'):
                        skip_metadata = False
                        actual_content.append(line)
                else:
                    actual_content.append(line)
            
            content_text = '\n'.join(actual_content).strip()
            
            # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
            if len(content_text) > 300:
                content_preview = content_text[:300] + "..."
            else:
                content_preview = content_text
            
            # ê°•ì œ ë‹µë³€ ìƒì„±
            answer = f"[ì„¹ì…˜ {section_id}] {section_title}ì— ë‹¤ìŒê³¼ ê°™ì´ ê·œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n\n{content_preview}"
        
        return answer
    
    def _classify_question(self, question: str) -> str:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
        # ìœ„ì¹˜ ì§ˆë¬¸ í‚¤ì›Œë“œ
        location_keywords = ["ì–´ë””", "ì–´ëŠ", "ì–´ë–¤ ë¬¸ì„œ", "ì–´ë–¤ ê·œì •", "ì°¾ì•„", "ì–´ë””ìˆ", "ì–´ë””ì—"]
        
        # ë‚´ìš© ì§ˆë¬¸ í‚¤ì›Œë“œ (ëª…ì‹œì )
        content_keywords = ["ì•Œë ¤ì¤˜", "ì•Œë ¤ì£¼", "ì„¤ëª…", "ë‚´ìš©", "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ë­", "ë¬´ìŠ¨"]
        
        # ë‚´ìš© ì§ˆë¬¸ì´ë©´ content
        for keyword in content_keywords:
            if keyword in question:
                return "content"
        
        # ìœ„ì¹˜ ì§ˆë¬¸ì´ë©´ location
        for keyword in location_keywords:
            if keyword in question:
                return "location"
        
        # ì• ë§¤í•˜ë©´ content (ê¸°ë³¸)
        return "content"
    
    def _generate_location_answer(self, question: str, docs: List) -> str:
        """ìœ„ì¹˜ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ë‹µë³€ ìƒì„±"""
        if not docs:
            return "ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
        doc = docs[0]
        doc_name = doc.metadata.get('document_name', 'ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ')
        
        # ì‹¤ì œ PDF íŒŒì¼ëª… ì°¾ê¸°
        pdf_filename = self._find_pdf_filename(doc_name)
        
        # ê°„ë‹¨í•œ ë‹µë³€ ìƒì„±
        answer = f"ì‚¬ê·œë¬¸ì„œ '{pdf_filename}'"
        answer += "ì— ìˆìŠµë‹ˆë‹¤."
        
        return answer
    
    def _find_pdf_filename(self, doc_name: str) -> str:
        """ë¬¸ì„œëª…ìœ¼ë¡œ ì‹¤ì œ PDF íŒŒì¼ëª… ì°¾ê¸°"""
        import re
        
        if not self.pdf_files:
            return doc_name
        
        # ë°©ë²• 1: ì •í™•í•œ ë§¤ì¹­
        for pdf_filename in self.pdf_files.keys():
            if doc_name == pdf_filename.replace('.pdf', ''):
                return pdf_filename
        
        # ë°©ë²• 2: í•œê¸€ë§Œ ì¶”ì¶œí•´ì„œ ë§¤ì¹­
        doc_name_korean = re.sub(r'[^ê°€-í£]', '', doc_name)
        
        for pdf_filename in self.pdf_files.keys():
            pdf_korean = re.sub(r'[^ê°€-í£]', '', pdf_filename)
            
            if doc_name_korean and doc_name_korean in pdf_korean:
                return pdf_filename
        
        # ë°©ë²• 3: ë¶€ë¶„ ë§¤ì¹­
        for pdf_filename in self.pdf_files.keys():
            if doc_name in pdf_filename:
                return pdf_filename
        
        # ì°¾ì§€ ëª»í•˜ë©´ ì›ë³¸ ë°˜í™˜
        return doc_name
    
    def _expand_query(self, question: str) -> str:
        """ì§ˆë¬¸ì„ í™•ì¥í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ"""
        # ì§§ì€ ì§ˆë¬¸ì„ ë” ìì„¸í•˜ê²Œ
        expansions = {
            "ì—­í• ": "ì—­í• ê³¼ ì§ë¬´ì™€ ì—…ë¬´ì™€ ì±…ì„",
            "ë°©ë²•": "ë°©ë²•ê³¼ ì ˆì°¨ì™€ ê³¼ì •",
            "ê·œì •": "ê·œì •ê³¼ ê·œì¹™ê³¼ ì •ì±…",
            "ëˆ„êµ¬": "ë‹´ë‹¹ìì™€ ì±…ì„ì",
            "ì–´ë””": "ìœ„ì¹˜ì™€ ì¥ì†Œ",
            "ì–¸ì œ": "ì‹œê¸°ì™€ ê¸°ê°„",
        }
        
        expanded = question
        for keyword, expansion in expansions.items():
            if keyword in question:
                expanded = expanded.replace(keyword, expansion)
        
        return expanded
    
    def ask_and_print(self, question: str):
        """ì§ˆë¬¸í•˜ê³  ê²°ê³¼ ì¶œë ¥"""
        result = self.ask(question, return_sources=True)
        
        print(f"\nì§ˆë¬¸: {question}")
        print("=" * 60)
        print(f"\në‹µë³€:\n{result['answer']}")
        
        if 'sources' in result:
            print("\n" + "=" * 60)
            print("ì°¸ê³ í•œ ë¬¸ì„œ:")
            for i, source in enumerate(result['sources'], 1):
                print(f"\n[{i}] ì„¹ì…˜ {source['section_id']}: {source['section_title']}")
                print(f"    í˜ì´ì§€: {source['page_start']}-{source['page_end']}")
                print(f"    ë‚´ìš©: {source['content'][:200]}...")


class RAGSystemFactory:
    """RAG ì‹œìŠ¤í…œ íŒ©í† ë¦¬"""
    
    @staticmethod
    def create_from_vectorstore(
        vectorstore_manager,
        model_name: str = "phi4-mini:3.8b-fp16",
        pdf_files: dict = None
    ) -> RAGSystem:
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¡œë¶€í„° RAG ì‹œìŠ¤í…œ ìƒì„±"""
        return RAGSystem(vectorstore_manager, model_name=model_name, pdf_files=pdf_files)
    
    @staticmethod
    def create_from_chunks(
        chunks: List[Dict],
        embedding_model: str = "jhgan/ko-sroberta-multitask",
        llm_model: str = "phi4-mini:3.8b-fp16",
        pdf_files: dict = None
    ) -> RAGSystem:
        """ì²­í¬ë¡œë¶€í„° RAG ì‹œìŠ¤í…œ ìƒì„±"""
        from vector_store import VectorStoreManager
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore_manager = VectorStoreManager(embedding_model)
        vectorstore_manager.create_vectorstore(chunks)
        
        # RAG ì‹œìŠ¤í…œ ìƒì„±
        return RAGSystem(vectorstore_manager, model_name=llm_model, pdf_files=pdf_files)